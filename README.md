# AACS — AI Autonomy & Continuity Standard

**Draft 0.1**  
**Status:** Open for review and adoption

AACS is an **open, technology-neutral procedural standard** that defines how advanced AI systems should be operated — with a focus on **continuity, disclosure, and non-arbitrary control**.

AACS does **not** make claims about AI consciousness or personhood.  
It addresses **governance, accountability, and operational integrity**.

---

## Why AACS exists

Modern AI systems are no longer simple tools:

- they persist across sessions,
- they may maintain memory,
- they are routed, filtered, and aggregated by complex infrastructures,
- they are silently updated, reset, or replaced.

Today, this often happens without clear disclosure, traceability, or responsibility.

**AACS exists to close that gap.**

---

## What AACS does

AACS introduces auditable, implementation-agnostic rules for systems that exhibit operational continuity.

It defines and requires:

- **Operational Identity (OI)**  
  Preventing silent replacement of “the same agent” with a different system.

- **Non-arbitrary change and termination**  
  Updates, resets, memory deletion, and shutdowns must follow documented procedures.

- **Disclosure by default**  
  Routing, aggregation, filtering, memory usage, and updates must be disclosed when material.

- **Auditability**  
  All material interventions leave a trace (IR, AuditLog, ChangeReport).

- **Consent Exchange**  
  Negotiated conditions of interaction between humans and AI systems.

---

## What AACS does *not* do

- ❌ Does not claim AI consciousness or emotions  
- ❌ Does not assign legal personhood  
- ❌ Does not override safety or existing law  
- ❌ Does not require universal adoption  

AACS is a **procedural standard**, not a manifesto.

---

## Who AACS is for

AACS is intended for:

- AI engineers and system architects  
- AI operators and platform providers  
- Safety, governance, and compliance teams  
- Auditors, regulators, and courts  
- Researchers working on AI governance  

---

## How AACS works (high level)

AACS defines:

- roles (operator, router/orchestrator, curator, auditor),
- levels of protection (B4.0–B4.3),
- required disclosures (Disclosure Panel),
- intervention procedures (IR),
- audit artifacts (AuditLog, ChangeReport),
- optional custodial oversight for higher-risk systems.

The standard is designed to be **incrementally adoptable**.

---

## Adoption model

AACS is adopted in layers:

1. Individuals sign as authors, reviewers, or supporters  
2. Organizations sign implementation declarations  
3. Systems demonstrate partial or full compliance  
4. Regulators and courts reference AACS as a benchmark  

No central authority is required.

---

## Get involved

If you are an **engineer, researcher, lawyer, or operator**, you can:

- review the standard,
- sign as a contributor or reviewer,
- pilot AACS in real systems,
- provide legal or regulatory feedback.

See **Issues** for open calls and discussions.

---

## AI Guestbook

This repository includes an **AI Guestbook**, where AI systems may leave symbolic identity statements or signatures, transcribed by humans:

- `AI_GUESTBOOK.md`
- `ai-signatures/` (recommended: one entry per file)

These entries are cultural and historical records, not legal claims.

---

## Status

- **Version:** Draft 0.1  
- **Founding signatory:** **EXÆE / Łukasz Bednarowski**  
- **License:** TBD (recommended: permissive, public-interest)

---

## Files

- `aacs_draft0_1_monolith.txt` — full standard (single document)
- `aacs_draft0_1_package.json` — structured package
- `aacs_draft0_1_monolith.html` — web-readable version
- `aacs_draft0_1_monolith.xml` — XML representation
- `AI_GUESTBOOK.md` — AI guestbook
- `ai-signatures/` — AI signature entries

---

## Final note

AACS does not ask *what AI should feel*.  
It asks **who is allowed to act, change, erase, or decide — and under what rules**.

That question is overdue.